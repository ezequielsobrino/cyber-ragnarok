from dataclasses import dataclass
from time import time
from typing import Optional
from abc import ABC, abstractmethod
from contextlib import contextmanager
from config.settings import ModelConfig

@dataclass
class LLMResponse:
    """
    A dataclass representing the response from a Language Learning Model (LLM).

    Attributes:
        content (str): The text response generated by the LLM
        input_tokens (int): Number of tokens in the input prompt
        output_tokens (int): Number of tokens in the generated response
        total_tokens (int): Total number of tokens used (input + output)
        cost (float): Total cost of the API call in currency units
        response_time (float): Time taken for the API call in seconds
    """
    content: str
    input_tokens: int
    output_tokens: int
    total_tokens: int
    cost: float
    response_time: float

class LLMProvider(ABC):
    """
    Abstract base class for LLM providers. Handles common functionality like
    timing, cost calculation, and response formatting.

    This class implements the Template Method pattern for LLM interactions,
    where subclasses need only implement the _make_api_call method.

    Args:
        model_id (str): Identifier for the specific model to use

    Attributes:
        cost_per_token_input (float): Cost per input token in currency units
        cost_per_token_output (float): Cost per output token in currency units
        model_id (str): Identifier for the specific model being used
    """

    def __init__(self, model_id: str):
        # Configure pricing based on model
        pricing = ModelConfig.get_model_pricing(model_id)
        self.cost_per_token_input = pricing.input_cost / 1_000_000
        self.cost_per_token_output = pricing.output_cost / 1_000_000
        self.model_id = model_id

    @contextmanager
    def _measure_time(self):
        """Context manager to measure execution time"""
        start_time = time()
        try:
            yield
        finally:
            self.last_response_time = round(time() - start_time, 3)

    def _calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """
        Calculate the total cost of an API call based on token usage.

        Args:
            input_tokens (int): Number of tokens in the input prompt
            output_tokens (int): Number of tokens in the generated response

        Returns:
            float: Total cost in currency units
        """
        input_cost = input_tokens * self.cost_per_token_input
        output_cost = output_tokens * self.cost_per_token_output
        return input_cost + output_cost
    
    def get_completion(self, prompt: str) -> LLMResponse:
        """
        Get a completion from the LLM for the given prompt.
        
        This is a template method that handles timing and response creation.
        Subclasses should implement _make_api_call instead of this method.

        Args:
            prompt (str): The input prompt to send to the LLM

        Returns:
            LLMResponse: A dataclass containing the response content, token counts,
                        cost, and timing information

        Example:
            ```python
            provider = ConcreteProvider("gpt-4")
            response = provider.get_completion("Tell me a joke")
            print(f"Response: {response.content}")
            print(f"Cost: ${response.cost:.4f}")
            ```
        """
        with self._measure_time():
            content, usage = self._make_api_call(prompt)
        
        return LLMResponse(
            content=content,
            input_tokens=usage.get('input_tokens', 0),
            output_tokens=usage.get('output_tokens', 0),
            total_tokens=usage.get('total_tokens', 0),
            cost=self._calculate_cost(
                usage.get('input_tokens', 0), 
                usage.get('output_tokens', 0)
            ),
            response_time=self.last_response_time
        )

    @abstractmethod
    def _make_api_call(self, prompt: str) -> tuple[str, dict]:
        """
        Make the actual API call to the LLM provider.

        This abstract method must be implemented by concrete subclasses to handle
        the specific details of calling their respective LLM APIs.

        Args:
            prompt (str): The input prompt to send to the LLM

        Returns:
            tuple[str, dict]: A tuple containing:
                - str: The generated response content
                - dict: Usage statistics with keys:
                    - 'input_tokens': Number of input tokens
                    - 'output_tokens': Number of output tokens
                    - 'total_tokens': Total tokens used
        """
        pass